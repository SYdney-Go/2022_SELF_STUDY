{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "t = np.array([0., 1., 2., 3., 4., 5., 6.])\n",
    "\n",
    "print(t)\n",
    "print(\"Rank of t : \", t.ndim)\n",
    "print(\"Shape of t : \", t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.array([[1., 2., 3.], [4., 5., 6.], [7., 8., 9.], [10., 11., 12.]])\n",
    "\n",
    "print(t)\n",
    "print(\"Rank of t :\", t.ndim)    \n",
    "print(\"Shape of t : \", t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 1., 2., 3., 4., 5., 6.])\n",
      "1\n",
      "torch.Size([7])\n",
      "torch.Size([7])\n",
      "<class 'torch.Tensor'>\n",
      "tensor(0.) tensor(1.) tensor(6.)\n",
      "tensor([2., 3., 4.]) tensor([4., 5.])\n",
      "tensor([0., 1.]) tensor([3., 4., 5., 6.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "t = torch.FloatTensor([0., 1., 2., 3., 4., 5., 6.])\n",
    "print(t)\n",
    "print(t.dim())\n",
    "print(t.shape)\n",
    "print(t.size())\n",
    "print(type(t))\n",
    "\n",
    "print(t[0], t[1], t[-1])\n",
    "print(t[2:5], t[4:-1])\n",
    "print(t[:2], t[3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-3.6862e+36,  4.5773e-41,  1.1986e-35,  0.0000e+00,  1.4013e-45,\n",
      "         0.0000e+00,  0.0000e+00])\n",
      "1\n",
      "torch.Size([7])\n",
      "torch.Size([7])\n"
     ]
    }
   ],
   "source": [
    "t = torch.Tensor(7)\n",
    "\n",
    "print(t)\n",
    "print(t.dim())\n",
    "print(t.shape)\n",
    "print(t.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 1., 2., 3., 4., 5., 6.], device='cuda:0')\n",
      "1\n",
      "torch.Size([7])\n",
      "torch.Size([7])\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "t = torch.cuda.FloatTensor([0., 1., 2., 3., 4., 5., 6.])\n",
    "\n",
    "print(t)\n",
    "print(t.dim())\n",
    "print(t.shape)\n",
    "print(t.size())\n",
    "print(type(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3 4] <class 'numpy.ndarray'>\n",
      "tensor([1., 2., 3., 4.]) <class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "t1 = np.array([1, 2, 3, 4])\n",
    "t2 = torch.Tensor(t1)\n",
    "\n",
    "print(t1, type(t1))\n",
    "print(t2, type(t2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6114, 0.0674, 0.4300],\n",
      "        [0.3737, 0.5329, 0.1998],\n",
      "        [0.5328, 0.8768, 0.3145]]) <class 'torch.Tensor'>\n",
      "[[0.61136174 0.06737643 0.429964  ]\n",
      " [0.37373132 0.532862   0.19983405]\n",
      " [0.5327635  0.8767636  0.31454396]] <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "t1 = torch.rand(3, 3)\n",
    "t2 = t1.numpy()\n",
    "\n",
    "print(t1, type(t1))\n",
    "print(t2, type(t2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-0.5026, -1.0684, -1.0123],\n",
      "          [ 0.2076,  0.3000,  0.6204],\n",
      "          [ 1.2220,  0.8290,  0.0368]]]])\n",
      "tensor([[[[-0.3171,  0.1182, -2.0305],\n",
      "          [ 1.0928,  1.5061, -0.2086],\n",
      "          [ 0.7133,  1.0139, -2.2112]]]])\n",
      "tensor([[[[-0.5026, -1.0684, -1.0123],\n",
      "          [ 0.2076,  0.3000,  0.6204],\n",
      "          [ 1.2220,  0.8290,  0.0368]]],\n",
      "\n",
      "\n",
      "        [[[-0.3171,  0.1182, -2.0305],\n",
      "          [ 1.0928,  1.5061, -0.2086],\n",
      "          [ 0.7133,  1.0139, -2.2112]]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "t1 = torch.randn(1, 1, 3, 3)\n",
    "t2 = torch.randn(1, 1, 3, 3)\n",
    "t3 = torch.cat((t1, t2), 0)\n",
    "\n",
    "print(t1)\n",
    "print(t2)\n",
    "print(t3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.],\n",
      "        [ 7.,  8.,  9.],\n",
      "        [10., 11., 12.]])\n",
      "2\n",
      "torch.Size([4, 3])\n",
      "tensor([ 2.,  5.,  8., 11.])\n",
      "torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "t = torch.FloatTensor([[1., 2., 3.],\n",
    "                       [4., 5., 6.],\n",
    "                       [7., 8., 9.],\n",
    "                       [10., 11., 12.]])\n",
    "\n",
    "print(t)\n",
    "print(t.dim())\n",
    "print(t.size())\n",
    "print(t[:, 1])\n",
    "print(t[:, 1].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5279, 1.6061, 1.1269],\n",
      "        [1.4931, 1.1967, 0.6613],\n",
      "        [1.0208, 0.7873, 0.9840]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.rand(3, 3)\n",
    "y = torch.rand(3, 3)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    x = x.cuda()\n",
    "    y = y.cuda()\n",
    "    sum = x + y\n",
    "    print(sum)\n",
    "    \n",
    "else:\n",
    "    print(\"GPU 지원 안됨!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3506) tensor(3.1550)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.rand(3, 3)\n",
    "x_mean = x.mean()\n",
    "x_sum = x.sum()\n",
    "\n",
    "print(x_mean, x_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Matrix 1 :  torch.Size([2, 2])\n",
      "Shape of Matrix 2 :  torch.Size([2, 1])\n",
      "tensor([[ 5.],\n",
      "        [11.]])\n"
     ]
    }
   ],
   "source": [
    "m1 = torch.FloatTensor([[1, 2], [3, 4]])\n",
    "m2 = torch.FloatTensor([[1], [2]])\n",
    "\n",
    "print(\"Shape of Matrix 1 : \", m1.shape)\n",
    "print(\"Shape of Matrix 2 : \", m2.shape)\n",
    "print(m1.matmul(m2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Matrix 1 :  torch.Size([2, 2])\n",
      "Shape of Matrix 2 :  torch.Size([2, 1])\n",
      "tensor([[1., 2.],\n",
      "        [6., 8.]])\n",
      "tensor([[1., 2.],\n",
      "        [6., 8.]])\n"
     ]
    }
   ],
   "source": [
    "m1 = torch.FloatTensor(([1, 2], [3, 4]))\n",
    "m2 = torch.FloatTensor(([1], [2]))\n",
    "\n",
    "print(\"Shape of Matrix 1 : \", m1.shape)\n",
    "print(\"Shape of Matrix 2 : \", m2.shape)\n",
    "print(m1 * m2)\n",
    "print(m1.mul(m2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5., 5.]])\n",
      "tensor([[4., 5.]])\n",
      "tensor([[4., 5.],\n",
      "        [5., 6.]])\n"
     ]
    }
   ],
   "source": [
    "# 크기가 같은 텐서의 연산\n",
    "m1 = torch.FloatTensor([[3, 3]])\n",
    "m2 = torch.FloatTensor([[2, 2]])\n",
    "print(m1 + m2)\n",
    "\n",
    "# 크기가 다른 텐서의 연산\n",
    "m1 = torch.FloatTensor([[1, 2]])\n",
    "m2 = torch.FloatTensor([3])\n",
    "print(m1 + m2)\n",
    "\n",
    "# 벡터 간 연산에서 브로드캐스팅\n",
    "m1 = torch.FloatTensor([[1, 2]])\n",
    "m2 = torch.FloatTensor([[3], [4]])\n",
    "print(m1 + m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 3])\n",
      "tensor([[ 0.,  1.,  2.],\n",
      "        [ 3.,  4.,  5.],\n",
      "        [ 6.,  7.,  8.],\n",
      "        [ 9., 10., 11.]])\n",
      "torch.Size([4, 3])\n"
     ]
    }
   ],
   "source": [
    "t = np.array([[[0, 1, 2],\n",
    "              [3, 4, 5],\n",
    "              [6, 7, 8],\n",
    "              [9, 10, 11]]])\n",
    "\n",
    "ft = torch.FloatTensor(t)\n",
    "print(ft.shape)\n",
    "\n",
    "print(ft.view([-1, 3]))\n",
    "print(ft.view([-1, 3]).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n",
      "None\n",
      "=========================================\n",
      "tensor([[3., 3.],\n",
      "        [3., 3.]], grad_fn=<AddBackward0>)\n",
      "=========================================\n",
      "tensor([[9., 9.],\n",
      "        [9., 9.]], grad_fn=<PowBackward0>)\n",
      "=========================================\n",
      "tensor(36., grad_fn=<SumBackward0>)\n",
      "=========================================\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n",
      "tensor([[6., 6.],\n",
      "        [6., 6.]])\n",
      "None\n",
      "=========================================\n",
      "tensor([[3., 3.],\n",
      "        [3., 3.]])\n",
      "None\n",
      "<AddBackward0 object at 0x7f99d0190f70>\n",
      "=========================================\n",
      "tensor([[9., 9.],\n",
      "        [9., 9.]])\n",
      "None\n",
      "<PowBackward0 object at 0x7f99d0190f70>\n",
      "=========================================\n",
      "tensor(36.)\n",
      "None\n",
      "<SumBackward0 object at 0x7f99d0190f70>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sydney/Desktop/2022_SELF_STUDY/딥러닝파이토치교과서/DeepLearningPytorch_venv/lib/python3.8/site-packages/torch/_tensor.py:1104: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at  aten/src/ATen/core/TensorBody.h:475.)\n",
      "  return self._grad\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "x = torch.ones(2, 2)\n",
    "x = Variable(x, requires_grad=True)\n",
    "print(x)\n",
    "print(x.data)\n",
    "print(x.grad_fn)\n",
    "print(\"=========================================\")\n",
    "\n",
    "y = x + 2\n",
    "print(y)\n",
    "print(\"=========================================\")\n",
    "\n",
    "z = y ** 2\n",
    "print(z)\n",
    "print(\"=========================================\")\n",
    "\n",
    "out = z.sum()\n",
    "print(out)\n",
    "print(\"=========================================\")\n",
    "\n",
    "out.backward()\n",
    "print(x.data)\n",
    "print(x.grad)\n",
    "print(x.grad_fn)\n",
    "print(\"=========================================\")\n",
    "\n",
    "print(y.data)\n",
    "print(y.grad)\n",
    "print(y.grad_fn)\n",
    "print(\"=========================================\")\n",
    "\n",
    "print(z.data)\n",
    "print(z.grad)\n",
    "print(z.grad_fn)\n",
    "print(\"=========================================\")\n",
    "\n",
    "print(out.data)\n",
    "print(out.grad)\n",
    "print(out.grad_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3., 3., 3.], grad_fn=<MulBackward0>)\n",
      "-----x.data-----\n",
      "tensor([1., 1., 1.])\n",
      "-----x.grad-----\n",
      "tensor([ 0.6000,  6.0000, 60.0000])\n",
      "-----x.grad_fn-----\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "x = torch.ones(3)\n",
    "x = Variable(x, requires_grad=True)\n",
    "y = x ** 2\n",
    "z = y * 3\n",
    "print(z)\n",
    "\n",
    "grad = torch.Tensor([0.1, 1, 10])\n",
    "z.backward(grad)\n",
    "\n",
    "print(\"-----x.data-----\")\n",
    "print(x.data)\n",
    "\n",
    "print(\"-----x.grad-----\")\n",
    "print(x.grad)\n",
    "\n",
    "print(\"-----x.grad_fn-----\")\n",
    "print(x.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 16, 50, 100])\n",
      "torch.Size([20, 33, 26, 101])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "m = nn.Conv2d(16, 22, 2, stride=2)\n",
    "m = nn.Conv2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2))\n",
    "m = nn.Conv2d(16, 33, (3, 4), stride=(2, 1), padding=(4, 2), dilation = (3, 1))\n",
    "\n",
    "input = torch.randn(20, 16, 50, 100)\n",
    "output = m(input)\n",
    "print(input.shape)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "import torch.autograd as autograd\n",
    "import torch.nn.functional as F\n",
    "\n",
    "filters = autograd.Variable(torch.randn(8, 4, 3, 3))\n",
    "inputs = autograd.Variable(torch.randn(1, 4, 5, 5))\n",
    "result = F.conv2d(inputs, filters, padding=1)\n",
    "print(result.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]]]], requires_grad=True)\n",
      "<ConvolutionBackward0 object at 0x7f991a2277c0>\n",
      "<ConvolutionBackward0 object at 0x7f991a2277c0>\n",
      "tensor([[[[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "input = torch.ones(1, 1, 3, 3)\n",
    "filter = torch.ones(1, 1, 3, 3)\n",
    "input = Variable(input, requires_grad=True)\n",
    "print(input)\n",
    "\n",
    "filter = Variable(filter, requires_grad=True)\n",
    "out = F.conv2d(input, filter)\n",
    "print(out.grad_fn)\n",
    "\n",
    "out.backward()\n",
    "print(out.grad_fn)\n",
    "print(input.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.0123]]]], grad_fn=<ConvolutionBackward0>)\n",
      "None\n",
      "<ConvolutionBackward0 object at 0x7f99d050e310>\n",
      "tensor([[[[-0.2758,  0.3023,  0.1220],\n",
      "          [-0.2933,  0.1815,  0.0030],\n",
      "          [ 0.1870,  0.2389, -0.2119]]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "\n",
    "input = torch.ones(1, 1, 3, 3)\n",
    "input = Variable(input, requires_grad=True)\n",
    "filter = nn.Conv2d(1, 1, 3)\n",
    "filter.weight\n",
    "out = filter(input)\n",
    "print(out)\n",
    "print(input.grad)\n",
    "\n",
    "out.backward()\n",
    "print(out.grad_fn)\n",
    "print(input.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[[[-0.2789, -0.2687, -0.2579],\n",
      "          [ 0.2115, -0.2454,  0.3047],\n",
      "          [-0.2023,  0.1667, -0.0384]]]], requires_grad=True)\n",
      "tensor([[[[18., 18., 18.],\n",
      "          [18., 18., 18.],\n",
      "          [18., 18., 18.]]]], grad_fn=<ConvolutionBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "input = torch.ones(1, 1 ,5, 5)\n",
    "input = Variable(input, requires_grad=True)\n",
    "filter = nn.Conv2d(1, 1, 3, bias=None)\n",
    "print(filter.weight)\n",
    "\n",
    "filter.weight = nn.Parameter(torch.ones(1, 1, 3, 3) +1)\n",
    "filter.weight\n",
    "out = filter(input)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.autograd.profiler as profiler\n",
    "\n",
    "class MyModule(nn.Module):\n",
    "    def __init__(self, in_features: int, out_features: int, bias: bool = True):\n",
    "        super(MyModule, self).__init__()\n",
    "        self.linear = nn.Linear(in_features, out_features, bias)\n",
    "\n",
    "    def forward(self, input, mask):\n",
    "        with profiler.record_function(\"LINEAR PASS\"):\n",
    "            out = self.linear(input)\n",
    "        with profiler.record_function(\"MASK INDICES\"):\n",
    "            threshold = out.sum(axis=1).mean().item()\n",
    "            hi_idx = np.argwhere(mask.cpu().numpy() > threshold)\n",
    "            hi_idx = torch.from_numpy(hi_idx).cuda()\n",
    "\n",
    "        return out, hi_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize(0.5, 0.5, 0.5), (0.5, 0.5, 0.5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.CenterCrop(10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1.4%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/sydney/Desktop/2022_SELF_STUDY/KETI/JetsonNano_CNN/torch_basic.ipynb Cell 26'\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/sydney/Desktop/2022_SELF_STUDY/KETI/JetsonNano_CNN/torch_basic.ipynb#ch0000026?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorchvision\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtransforms\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtransforms\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/sydney/Desktop/2022_SELF_STUDY/KETI/JetsonNano_CNN/torch_basic.ipynb#ch0000026?line=3'>4</a>\u001b[0m transform \u001b[39m=\u001b[39m transforms\u001b[39m.\u001b[39mCompose([transforms\u001b[39m.\u001b[39mToTensor(),\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/sydney/Desktop/2022_SELF_STUDY/KETI/JetsonNano_CNN/torch_basic.ipynb#ch0000026?line=4'>5</a>\u001b[0m                                 transforms\u001b[39m.\u001b[39mNormalize((\u001b[39m0.5\u001b[39m, \u001b[39m0.5\u001b[39m, \u001b[39m0.5\u001b[39m, \u001b[39m0.5\u001b[39m), (\u001b[39m0.5\u001b[39m, \u001b[39m0.5\u001b[39m, \u001b[39m0.5\u001b[39m, \u001b[39m0.5\u001b[39m))])\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/sydney/Desktop/2022_SELF_STUDY/KETI/JetsonNano_CNN/torch_basic.ipynb#ch0000026?line=6'>7</a>\u001b[0m trainset \u001b[39m=\u001b[39m torchvision\u001b[39m.\u001b[39;49mdatasets\u001b[39m.\u001b[39;49mCIFAR10(root\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m./\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/sydney/Desktop/2022_SELF_STUDY/KETI/JetsonNano_CNN/torch_basic.ipynb#ch0000026?line=7'>8</a>\u001b[0m                                         train\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/sydney/Desktop/2022_SELF_STUDY/KETI/JetsonNano_CNN/torch_basic.ipynb#ch0000026?line=8'>9</a>\u001b[0m                                         download\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sydney/Desktop/2022_SELF_STUDY/KETI/JetsonNano_CNN/torch_basic.ipynb#ch0000026?line=9'>10</a>\u001b[0m                                         transform\u001b[39m=\u001b[39;49mtransform)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sydney/Desktop/2022_SELF_STUDY/KETI/JetsonNano_CNN/torch_basic.ipynb#ch0000026?line=11'>12</a>\u001b[0m testset \u001b[39m=\u001b[39m torchvision\u001b[39m.\u001b[39mdatasets\u001b[39m.\u001b[39mCIFAR10(root\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m./\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sydney/Desktop/2022_SELF_STUDY/KETI/JetsonNano_CNN/torch_basic.ipynb#ch0000026?line=12'>13</a>\u001b[0m                                        train\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sydney/Desktop/2022_SELF_STUDY/KETI/JetsonNano_CNN/torch_basic.ipynb#ch0000026?line=13'>14</a>\u001b[0m                                        download\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sydney/Desktop/2022_SELF_STUDY/KETI/JetsonNano_CNN/torch_basic.ipynb#ch0000026?line=14'>15</a>\u001b[0m                                        transform\u001b[39m=\u001b[39mtransform)\n",
      "File \u001b[0;32m~/Desktop/2022_SELF_STUDY/딥러닝파이토치교과서/DeepLearningPytorch_venv/lib/python3.8/site-packages/torchvision/datasets/cifar.py:65\u001b[0m, in \u001b[0;36mCIFAR10.__init__\u001b[0;34m(self, root, train, transform, target_transform, download)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain \u001b[39m=\u001b[39m train  \u001b[39m# training set or test set\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mif\u001b[39;00m download:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdownload()\n\u001b[1;32m     67\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_integrity():\n\u001b[1;32m     68\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mDataset not found or corrupted. You can use download=True to download it\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/2022_SELF_STUDY/딥러닝파이토치교과서/DeepLearningPytorch_venv/lib/python3.8/site-packages/torchvision/datasets/cifar.py:141\u001b[0m, in \u001b[0;36mCIFAR10.download\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mFiles already downloaded and verified\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    140\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m--> 141\u001b[0m download_and_extract_archive(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49murl, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mroot, filename\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfilename, md5\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtgz_md5)\n",
      "File \u001b[0;32m~/Desktop/2022_SELF_STUDY/딥러닝파이토치교과서/DeepLearningPytorch_venv/lib/python3.8/site-packages/torchvision/datasets/utils.py:430\u001b[0m, in \u001b[0;36mdownload_and_extract_archive\u001b[0;34m(url, download_root, extract_root, filename, md5, remove_finished)\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m filename:\n\u001b[1;32m    428\u001b[0m     filename \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mbasename(url)\n\u001b[0;32m--> 430\u001b[0m download_url(url, download_root, filename, md5)\n\u001b[1;32m    432\u001b[0m archive \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(download_root, filename)\n\u001b[1;32m    433\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExtracting \u001b[39m\u001b[39m{\u001b[39;00marchive\u001b[39m}\u001b[39;00m\u001b[39m to \u001b[39m\u001b[39m{\u001b[39;00mextract_root\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/2022_SELF_STUDY/딥러닝파이토치교과서/DeepLearningPytorch_venv/lib/python3.8/site-packages/torchvision/datasets/utils.py:141\u001b[0m, in \u001b[0;36mdownload_url\u001b[0;34m(url, root, filename, md5, max_redirect_hops)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    140\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mDownloading \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m url \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m to \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m fpath)\n\u001b[0;32m--> 141\u001b[0m     _urlretrieve(url, fpath)\n\u001b[1;32m    142\u001b[0m \u001b[39mexcept\u001b[39;00m (urllib\u001b[39m.\u001b[39merror\u001b[39m.\u001b[39mURLError, \u001b[39mOSError\u001b[39;00m) \u001b[39mas\u001b[39;00m e:  \u001b[39m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    143\u001b[0m     \u001b[39mif\u001b[39;00m url[:\u001b[39m5\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhttps\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[0;32m~/Desktop/2022_SELF_STUDY/딥러닝파이토치교과서/DeepLearningPytorch_venv/lib/python3.8/site-packages/torchvision/datasets/utils.py:38\u001b[0m, in \u001b[0;36m_urlretrieve\u001b[0;34m(url, filename, chunk_size)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m chunk:\n\u001b[1;32m     37\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m pbar\u001b[39m.\u001b[39;49mupdate(chunk_size)\n\u001b[1;32m     39\u001b[0m fh\u001b[39m.\u001b[39mwrite(chunk)\n",
      "File \u001b[0;32m~/Desktop/2022_SELF_STUDY/딥러닝파이토치교과서/DeepLearningPytorch_venv/lib/python3.8/site-packages/torch/hub.py:42\u001b[0m, in \u001b[0;36mtqdm.update\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     41\u001b[0m     sys\u001b[39m.\u001b[39mstderr\u001b[39m.\u001b[39mwrite(\u001b[39m\"\u001b[39m\u001b[39m\\r\u001b[39;00m\u001b[39m{0:.1f}\u001b[39;00m\u001b[39m%\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39m100\u001b[39m \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn \u001b[39m/\u001b[39m \u001b[39mfloat\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtotal)))\n\u001b[0;32m---> 42\u001b[0m sys\u001b[39m.\u001b[39;49mstderr\u001b[39m.\u001b[39;49mflush()\n",
      "File \u001b[0;32m~/Desktop/2022_SELF_STUDY/딥러닝파이토치교과서/DeepLearningPytorch_venv/lib/python3.8/site-packages/ipykernel/iostream.py:480\u001b[0m, in \u001b[0;36mOutStream.flush\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    478\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpub_thread\u001b[39m.\u001b[39mschedule(evt\u001b[39m.\u001b[39mset)\n\u001b[1;32m    479\u001b[0m     \u001b[39m# and give a timeout to avoid\u001b[39;00m\n\u001b[0;32m--> 480\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m evt\u001b[39m.\u001b[39;49mwait(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mflush_timeout):\n\u001b[1;32m    481\u001b[0m         \u001b[39m# write directly to __stderr__ instead of warning because\u001b[39;00m\n\u001b[1;32m    482\u001b[0m         \u001b[39m# if this is happening sys.stderr may be the problem.\u001b[39;00m\n\u001b[1;32m    483\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mIOStream.flush timed out\u001b[39m\u001b[39m\"\u001b[39m, file\u001b[39m=\u001b[39msys\u001b[39m.\u001b[39m__stderr__)\n\u001b[1;32m    484\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/lib/python3.8/threading.py:558\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    556\u001b[0m signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flag\n\u001b[1;32m    557\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 558\u001b[0m     signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cond\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    559\u001b[0m \u001b[39mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m/usr/lib/python3.8/threading.py:306\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    305\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 306\u001b[0m         gotit \u001b[39m=\u001b[39m waiter\u001b[39m.\u001b[39;49macquire(\u001b[39mTrue\u001b[39;49;00m, timeout)\n\u001b[1;32m    307\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    308\u001b[0m         gotit \u001b[39m=\u001b[39m waiter\u001b[39m.\u001b[39macquire(\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5, 0.5, 0.5, 0.5), (0.5, 0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root=\"./\",\n",
    "                                        train=True,\n",
    "                                        download=True,\n",
    "                                        transform=transform)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root=\"./\",\n",
    "                                       train=False,\n",
    "                                       download=True,\n",
    "                                       transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'scipy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/home/sydney/Desktop/2022_SELF_STUDY/KETI/JetsonNano_CNN/torch_basic.ipynb Cell 27'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/sydney/Desktop/2022_SELF_STUDY/KETI/JetsonNano_CNN/torch_basic.ipynb#ch0000027?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m \u001b[39mimport\u001b[39;00m DataLoader\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/sydney/Desktop/2022_SELF_STUDY/KETI/JetsonNano_CNN/torch_basic.ipynb#ch0000027?line=2'>3</a>\u001b[0m imagenet_data \u001b[39m=\u001b[39m torchvision\u001b[39m.\u001b[39;49mdatasets\u001b[39m.\u001b[39;49mImageNet(\u001b[39m\"\u001b[39;49m\u001b[39m/\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/sydney/Desktop/2022_SELF_STUDY/KETI/JetsonNano_CNN/torch_basic.ipynb#ch0000027?line=3'>4</a>\u001b[0m data_loader \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataLoader(imagenet_data,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/sydney/Desktop/2022_SELF_STUDY/KETI/JetsonNano_CNN/torch_basic.ipynb#ch0000027?line=4'>5</a>\u001b[0m                                           batch_size\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/sydney/Desktop/2022_SELF_STUDY/KETI/JetsonNano_CNN/torch_basic.ipynb#ch0000027?line=5'>6</a>\u001b[0m                                           shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/sydney/Desktop/2022_SELF_STUDY/KETI/JetsonNano_CNN/torch_basic.ipynb#ch0000027?line=6'>7</a>\u001b[0m                                           num_worker\u001b[39m=\u001b[39margs\u001b[39m.\u001b[39mnThreads)\n",
      "File \u001b[0;32m~/Desktop/2022_SELF_STUDY/딥러닝파이토치교과서/DeepLearningPytorch_venv/lib/python3.8/site-packages/torchvision/datasets/imagenet.py:46\u001b[0m, in \u001b[0;36mImageNet.__init__\u001b[0;34m(self, root, split, **kwargs)\u001b[0m\n\u001b[1;32m     43\u001b[0m root \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mroot \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexpanduser(root)\n\u001b[1;32m     44\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msplit \u001b[39m=\u001b[39m verify_str_arg(split, \u001b[39m\"\u001b[39m\u001b[39msplit\u001b[39m\u001b[39m\"\u001b[39m, (\u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mval\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[0;32m---> 46\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparse_archives()\n\u001b[1;32m     47\u001b[0m wnid_to_classes \u001b[39m=\u001b[39m load_meta_file(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mroot)[\u001b[39m0\u001b[39m]\n\u001b[1;32m     49\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msplit_folder, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Desktop/2022_SELF_STUDY/딥러닝파이토치교과서/DeepLearningPytorch_venv/lib/python3.8/site-packages/torchvision/datasets/imagenet.py:59\u001b[0m, in \u001b[0;36mImageNet.parse_archives\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mparse_archives\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     58\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m check_integrity(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mroot, META_FILE)):\n\u001b[0;32m---> 59\u001b[0m         parse_devkit_archive(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mroot)\n\u001b[1;32m     61\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39misdir(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msplit_folder):\n\u001b[1;32m     62\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msplit \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[0;32m~/Desktop/2022_SELF_STUDY/딥러닝파이토치교과서/DeepLearningPytorch_venv/lib/python3.8/site-packages/torchvision/datasets/imagenet.py:108\u001b[0m, in \u001b[0;36mparse_devkit_archive\u001b[0;34m(root, file)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mparse_devkit_archive\u001b[39m(root: \u001b[39mstr\u001b[39m, file: Optional[\u001b[39mstr\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    100\u001b[0m     \u001b[39m\"\"\"Parse the devkit archive of the ImageNet2012 classification dataset and save\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[39m    the meta information in a binary file.\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[39m            'ILSVRC2012_devkit_t12.tar.gz'\u001b[39;00m\n\u001b[1;32m    107\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 108\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mio\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39msio\u001b[39;00m\n\u001b[1;32m    110\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mparse_meta_mat\u001b[39m(devkit_root: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Dict[\u001b[39mint\u001b[39m, \u001b[39mstr\u001b[39m], Dict[\u001b[39mstr\u001b[39m, Tuple[\u001b[39mstr\u001b[39m, \u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m]]]:\n\u001b[1;32m    111\u001b[0m         metafile \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(devkit_root, \u001b[39m\"\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmeta.mat\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'scipy'"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import argparse as args\n",
    "\n",
    "imagenet_data = torchvision.datasets.ImageNet(\"/\")\n",
    "data_loader = DataLoader(imagenet_data,\n",
    "                        batch_size=4,\n",
    "                        shuffle=True,\n",
    "                        num_worker=args.nThreads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (403691287.py, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [68]\u001b[0;36m\u001b[0m\n\u001b[0;31m    # forward 구조\u001b[0m\n\u001b[0m                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "class my_network(nn.Module):\n",
    "    def __int__(self):\n",
    "        super(my_network, self).__init__()\n",
    "        # 사용할 함수 정의\n",
    "    def forward(self, x):\n",
    "        # forward 구조"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0 / 2000 w: 0.187, b: 0.080 cost: -18.666667\n",
      "Epoch  100 / 2000 w: 1.746, b: 0.578 cost: -0.061424\n",
      "Epoch  200 / 2000 w: 1.800, b: 0.454 cost: -0.048171\n",
      "Epoch  300 / 2000 w: 1.843, b: 0.357 cost: -0.037866\n",
      "Epoch  400 / 2000 w: 1.876, b: 0.281 cost: -0.029766\n",
      "Epoch  500 / 2000 w: 1.903, b: 0.221 cost: -0.023399\n",
      "Epoch  600 / 2000 w: 1.924, b: 0.174 cost: -0.018394\n",
      "Epoch  700 / 2000 w: 1.940, b: 0.136 cost: -0.014459\n",
      "Epoch  800 / 2000 w: 1.953, b: 0.107 cost: -0.011366\n",
      "Epoch  900 / 2000 w: 1.963, b: 0.084 cost: -0.008935\n",
      "Epoch 1000 / 2000 w: 1.971, b: 0.066 cost: -0.007024\n",
      "Epoch 1100 / 2000 w: 1.977, b: 0.052 cost: -0.005521\n",
      "Epoch 1200 / 2000 w: 1.982, b: 0.041 cost: -0.004340\n",
      "Epoch 1300 / 2000 w: 1.986, b: 0.032 cost: -0.003412\n",
      "Epoch 1400 / 2000 w: 1.989, b: 0.025 cost: -0.002682\n",
      "Epoch 1500 / 2000 w: 1.991, b: 0.020 cost: -0.002108\n",
      "Epoch 1600 / 2000 w: 1.993, b: 0.016 cost: -0.001657\n",
      "Epoch 1700 / 2000 w: 1.995, b: 0.012 cost: -0.001303\n",
      "Epoch 1800 / 2000 w: 1.996, b: 0.010 cost: -0.001024\n",
      "Epoch 1900 / 2000 w: 1.997, b: 0.008 cost: -0.000805\n",
      "Epoch 2000 / 2000 w: 1.997, b: 0.006 cost: -0.000633\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x_train = np.array([[1], [2], [3]])\n",
    "y_train = np.array([[2], [4], [6]])\n",
    "\n",
    "w = 0\n",
    "b = 0\n",
    "lr = 0.01\n",
    "n_data = len(x_train)\n",
    "nb_epochs = 2000\n",
    "\n",
    "for epoch in range(nb_epochs+1):\n",
    "    hypothesis = x_train * w + b\n",
    "    cost = np.sum((hypothesis - y_train) * 2 *x_train) / n_data\n",
    "    gradient_w = np.sum((hypothesis - y_train) * 2 * x_train) / n_data\n",
    "    gradient_b = np.sum((hypothesis - y_train) * 2) / n_data\n",
    "    w -= lr * gradient_w\n",
    "    b -= lr * gradient_b\n",
    "    if epoch % 100 == 0:\n",
    "        print(\"Epoch {:4d} / {} w: {:.3f}, b: {:.3f} cost: {:.6f}\".format(epoch, nb_epochs, w, b, cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "x_train = torch.FloatTensor([[1], [2], [3]])\n",
    "y_train = torch.FloatTensor([[2], [4], [6]])\n",
    "\n",
    "w = torch.zeros(1, requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "\n",
    "hypothesis = x_train * w + b\n",
    "cost = torch.mean((hypothesis - y_train) ** 2)\n",
    "print(cost)\n",
    "\n",
    "optimizer = optim.SGD([w, b], lr=0.01)\n",
    "optimizer.zero_grad()\n",
    "\n",
    "cost.backward()\n",
    "\n",
    "optimizer.step()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/2000 w: 0.187 b: 0.080 cost: 18.666666\n",
      "Epoch  100/2000 w: 1.746 b: 0.578 cost: 0.048171\n",
      "Epoch  200/2000 w: 1.800 b: 0.454 cost: 0.029767\n",
      "Epoch  300/2000 w: 1.843 b: 0.357 cost: 0.018394\n",
      "Epoch  400/2000 w: 1.876 b: 0.281 cost: 0.011366\n",
      "Epoch  500/2000 w: 1.903 b: 0.221 cost: 0.007024\n",
      "Epoch  600/2000 w: 1.924 b: 0.174 cost: 0.004340\n",
      "Epoch  700/2000 w: 1.940 b: 0.136 cost: 0.002682\n",
      "Epoch  800/2000 w: 1.953 b: 0.107 cost: 0.001657\n",
      "Epoch  900/2000 w: 1.963 b: 0.084 cost: 0.001024\n",
      "Epoch 1000/2000 w: 1.971 b: 0.066 cost: 0.000633\n",
      "Epoch 1100/2000 w: 1.977 b: 0.052 cost: 0.000391\n",
      "Epoch 1200/2000 w: 1.982 b: 0.041 cost: 0.000242\n",
      "Epoch 1300/2000 w: 1.986 b: 0.032 cost: 0.000149\n",
      "Epoch 1400/2000 w: 1.989 b: 0.025 cost: 0.000092\n",
      "Epoch 1500/2000 w: 1.991 b: 0.020 cost: 0.000057\n",
      "Epoch 1600/2000 w: 1.993 b: 0.016 cost: 0.000035\n",
      "Epoch 1700/2000 w: 1.995 b: 0.012 cost: 0.000022\n",
      "Epoch 1800/2000 w: 1.996 b: 0.010 cost: 0.000013\n",
      "Epoch 1900/2000 w: 1.997 b: 0.008 cost: 0.000008\n",
      "Epoch 2000/2000 w: 1.997 b: 0.006 cost: 0.000005\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "x_train = torch.FloatTensor([[1], [2], [3]])\n",
    "y_train = torch.FloatTensor([[2], [4], [6]])\n",
    "\n",
    "w = torch.zeros(1, requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "\n",
    "optimizer = optim.SGD([w, b], lr=0.01)\n",
    "\n",
    "nb_epochs = 2000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    hypothesis = x_train * w + b\n",
    "    cost = torch.mean((hypothesis - y_train) ** 2)\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    if epoch % 100 == 0 :\n",
    "        print(\"Epoch {:4d}/{} w: {:.3f} b: {:.3f} cost: {:.6f}\".format(epoch, nb_epochs, w.item(), b.item(), cost.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "수식을 w로 미분한 값 : 8.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "w = torch.tensor(2.0, requires_grad=True)\n",
    "y = w ** 2\n",
    "z = 2*y + 5\n",
    "\n",
    "z.backward()\n",
    "print(\"수식을 w로 미분한 값 : {}\".format(w.grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/500000 w1: 0.294  w2: 0.294  w3: 0.297 b: 0.003 cost: 29661.800781\n",
      "Epoch 10000/500000 w1: 0.888  w2: 0.464  w3: 0.658 b: 0.020 cost: 0.271223\n",
      "Epoch 20000/500000 w1: 0.933  w2: 0.466  w3: 0.612 b: 0.028 cost: 0.227067\n",
      "Epoch 30000/500000 w1: 0.960  w2: 0.478  w3: 0.574 b: 0.036 cost: 0.203368\n",
      "Epoch 40000/500000 w1: 0.980  w2: 0.488  w3: 0.544 b: 0.042 cost: 0.188761\n",
      "Epoch 50000/500000 w1: 0.996  w2: 0.496  w3: 0.521 b: 0.048 cost: 0.179741\n",
      "Epoch 60000/500000 w1: 1.008  w2: 0.502  w3: 0.502 b: 0.053 cost: 0.174160\n",
      "Epoch 70000/500000 w1: 1.018  w2: 0.507  w3: 0.488 b: 0.058 cost: 0.170693\n",
      "Epoch 80000/500000 w1: 1.025  w2: 0.510  w3: 0.477 b: 0.062 cost: 0.168531\n",
      "Epoch 90000/500000 w1: 1.031  w2: 0.513  w3: 0.468 b: 0.066 cost: 0.167169\n",
      "Epoch 100000/500000 w1: 1.036  w2: 0.516  w3: 0.461 b: 0.070 cost: 0.166304\n",
      "Epoch 110000/500000 w1: 1.039  w2: 0.517  w3: 0.456 b: 0.073 cost: 0.165734\n",
      "Epoch 120000/500000 w1: 1.042  w2: 0.519  w3: 0.452 b: 0.076 cost: 0.165365\n",
      "Epoch 130000/500000 w1: 1.044  w2: 0.520  w3: 0.448 b: 0.079 cost: 0.165096\n",
      "Epoch 140000/500000 w1: 1.046  w2: 0.521  w3: 0.446 b: 0.083 cost: 0.164909\n",
      "Epoch 150000/500000 w1: 1.047  w2: 0.521  w3: 0.444 b: 0.086 cost: 0.164766\n",
      "Epoch 160000/500000 w1: 1.048  w2: 0.522  w3: 0.443 b: 0.089 cost: 0.164638\n",
      "Epoch 170000/500000 w1: 1.049  w2: 0.522  w3: 0.441 b: 0.092 cost: 0.164534\n",
      "Epoch 180000/500000 w1: 1.049  w2: 0.522  w3: 0.441 b: 0.094 cost: 0.164444\n",
      "Epoch 190000/500000 w1: 1.049  w2: 0.523  w3: 0.440 b: 0.097 cost: 0.164357\n",
      "Epoch 200000/500000 w1: 1.050  w2: 0.523  w3: 0.440 b: 0.100 cost: 0.164269\n",
      "Epoch 210000/500000 w1: 1.050  w2: 0.523  w3: 0.440 b: 0.103 cost: 0.164192\n",
      "Epoch 220000/500000 w1: 1.050  w2: 0.523  w3: 0.440 b: 0.106 cost: 0.164111\n",
      "Epoch 230000/500000 w1: 1.050  w2: 0.523  w3: 0.440 b: 0.109 cost: 0.164035\n",
      "Epoch 240000/500000 w1: 1.050  w2: 0.523  w3: 0.440 b: 0.111 cost: 0.163951\n",
      "Epoch 250000/500000 w1: 1.050  w2: 0.523  w3: 0.440 b: 0.114 cost: 0.163876\n",
      "Epoch 260000/500000 w1: 1.050  w2: 0.523  w3: 0.440 b: 0.117 cost: 0.163795\n",
      "Epoch 270000/500000 w1: 1.050  w2: 0.523  w3: 0.440 b: 0.120 cost: 0.163719\n",
      "Epoch 280000/500000 w1: 1.050  w2: 0.523  w3: 0.440 b: 0.122 cost: 0.163648\n",
      "Epoch 290000/500000 w1: 1.050  w2: 0.523  w3: 0.440 b: 0.125 cost: 0.163569\n",
      "Epoch 300000/500000 w1: 1.050  w2: 0.523  w3: 0.440 b: 0.128 cost: 0.163495\n",
      "Epoch 310000/500000 w1: 1.050  w2: 0.523  w3: 0.440 b: 0.131 cost: 0.163424\n",
      "Epoch 320000/500000 w1: 1.050  w2: 0.523  w3: 0.440 b: 0.133 cost: 0.163350\n",
      "Epoch 330000/500000 w1: 1.050  w2: 0.523  w3: 0.440 b: 0.136 cost: 0.163275\n",
      "Epoch 340000/500000 w1: 1.050  w2: 0.523  w3: 0.440 b: 0.139 cost: 0.163204\n",
      "Epoch 350000/500000 w1: 1.049  w2: 0.523  w3: 0.440 b: 0.141 cost: 0.163130\n",
      "Epoch 360000/500000 w1: 1.049  w2: 0.523  w3: 0.440 b: 0.144 cost: 0.163053\n",
      "Epoch 370000/500000 w1: 1.049  w2: 0.522  w3: 0.440 b: 0.147 cost: 0.162986\n",
      "Epoch 380000/500000 w1: 1.049  w2: 0.522  w3: 0.440 b: 0.149 cost: 0.162912\n",
      "Epoch 390000/500000 w1: 1.049  w2: 0.522  w3: 0.440 b: 0.152 cost: 0.162830\n",
      "Epoch 400000/500000 w1: 1.049  w2: 0.522  w3: 0.441 b: 0.155 cost: 0.162756\n",
      "Epoch 410000/500000 w1: 1.048  w2: 0.522  w3: 0.441 b: 0.157 cost: 0.162686\n",
      "Epoch 420000/500000 w1: 1.048  w2: 0.522  w3: 0.441 b: 0.160 cost: 0.162608\n",
      "Epoch 430000/500000 w1: 1.048  w2: 0.522  w3: 0.441 b: 0.163 cost: 0.162535\n",
      "Epoch 440000/500000 w1: 1.048  w2: 0.522  w3: 0.441 b: 0.165 cost: 0.162464\n",
      "Epoch 450000/500000 w1: 1.048  w2: 0.522  w3: 0.442 b: 0.168 cost: 0.162394\n",
      "Epoch 460000/500000 w1: 1.048  w2: 0.522  w3: 0.442 b: 0.171 cost: 0.162318\n",
      "Epoch 470000/500000 w1: 1.048  w2: 0.522  w3: 0.442 b: 0.174 cost: 0.162248\n",
      "Epoch 480000/500000 w1: 1.047  w2: 0.522  w3: 0.442 b: 0.176 cost: 0.162168\n",
      "Epoch 490000/500000 w1: 1.047  w2: 0.522  w3: 0.442 b: 0.179 cost: 0.162093\n",
      "Epoch 500000/500000 w1: 1.047  w2: 0.522  w3: 0.443 b: 0.182 cost: 0.162023\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "x1_train = torch.FloatTensor([[73], [93], [89], [96], [73]])\n",
    "x2_train = torch.FloatTensor([[80], [88], [91], [98], [66]])\n",
    "x3_train = torch.FloatTensor([[75], [93], [90], [100], [70]])\n",
    "y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])\n",
    "\n",
    "w1 = torch.zeros(1, requires_grad=True)\n",
    "w2 = torch.zeros(1, requires_grad=True)\n",
    "w3 = torch.zeros(1, requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "\n",
    "optimizer = optim.SGD([w1, w2, w3, b], lr=1e-5)\n",
    "nb_epochs = 500000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    hypothesis = x1_train * w1 + x2_train * w2 + x3_train * w3 + b\n",
    "    cost = torch.mean((hypothesis - y_train) ** 2)\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 10000 == 0:\n",
    "        print(\"Epoch {:4d}/{} w1: {:.3f}  w2: {:.3f}  w3: {:.3f} b: {:.3f} cost: {:.6f}\".format(\n",
    "            epoch, nb_epochs, w1.item(), w2.item(), w3.item(), b.item(), cost.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 1])\n",
      "torch.Size([5, 1])\n",
      "Epoch    0/20 hypothesis: tensor([[0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.]]) cost: 29661.800781\n",
      "Epoch    1/20 hypothesis: tensor([[18.0378, 18.0378, 18.0378, 18.0378, 18.0378],\n",
      "        [21.9532, 21.9532, 21.9532, 21.9532, 21.9532],\n",
      "        [21.3599, 21.3599, 21.3599, 21.3599, 21.3599],\n",
      "        [23.2583, 23.2583, 23.2583, 23.2583, 23.2583],\n",
      "        [16.8513, 16.8513, 16.8513, 16.8513, 16.8513]]) cost: 23039.744141\n",
      "Epoch    2/20 hypothesis: tensor([[33.9351, 33.9351, 33.9351, 33.9351, 33.9351],\n",
      "        [41.3012, 41.3012, 41.3012, 41.3012, 41.3012],\n",
      "        [40.1851, 40.1851, 40.1851, 40.1851, 40.1851],\n",
      "        [43.7566, 43.7566, 43.7566, 43.7566, 43.7566],\n",
      "        [31.7029, 31.7029, 31.7029, 31.7029, 31.7029]]) cost: 17896.076172\n",
      "Epoch    3/20 hypothesis: tensor([[47.9459, 47.9459, 47.9459, 47.9459, 47.9459],\n",
      "        [58.3533, 58.3533, 58.3533, 58.3533, 58.3533],\n",
      "        [56.7764, 56.7764, 56.7764, 56.7764, 56.7764],\n",
      "        [61.8224, 61.8224, 61.8224, 61.8224, 61.8224],\n",
      "        [44.7922, 44.7922, 44.7922, 44.7922, 44.7922]]) cost: 13900.740234\n",
      "Epoch    4/20 hypothesis: tensor([[60.2941, 60.2941, 60.2941, 60.2941, 60.2941],\n",
      "        [73.3818, 73.3818, 73.3818, 73.3818, 73.3818],\n",
      "        [71.3988, 71.3988, 71.3988, 71.3988, 71.3988],\n",
      "        [77.7444, 77.7444, 77.7444, 77.7444, 77.7444],\n",
      "        [56.3282, 56.3282, 56.3282, 56.3282, 56.3282]]) cost: 10797.374023\n",
      "Epoch    5/20 hypothesis: tensor([[71.1770, 71.1770, 71.1770, 71.1770, 71.1770],\n",
      "        [86.6270, 86.6270, 86.6270, 86.6270, 86.6270],\n",
      "        [84.2861, 84.2861, 84.2861, 84.2861, 84.2861],\n",
      "        [91.7770, 91.7770, 91.7770, 91.7770, 91.7770],\n",
      "        [66.4952, 66.4952, 66.4952, 66.4952, 66.4952]]) cost: 8386.837891\n",
      "Epoch    6/20 hypothesis: tensor([[ 80.7684,  80.7684,  80.7684,  80.7684,  80.7684],\n",
      "        [ 98.3004,  98.3004,  98.3004,  98.3004,  98.3004],\n",
      "        [ 95.6440,  95.6440,  95.6440,  95.6440,  95.6440],\n",
      "        [104.1443, 104.1443, 104.1443, 104.1443, 104.1443],\n",
      "        [ 75.4557,  75.4557,  75.4557,  75.4557,  75.4557]]) cost: 6514.458984\n",
      "Epoch    7/20 hypothesis: tensor([[ 89.2217,  89.2217,  89.2217,  89.2217,  89.2217],\n",
      "        [108.5885, 108.5885, 108.5885, 108.5885, 108.5885],\n",
      "        [105.6542, 105.6542, 105.6542, 105.6542, 105.6542],\n",
      "        [115.0441, 115.0441, 115.0441, 115.0441, 115.0441],\n",
      "        [ 83.3530,  83.3530,  83.3530,  83.3530,  83.3530]]) cost: 5060.092773\n",
      "Epoch    8/20 hypothesis: tensor([[ 96.6718,  96.6718,  96.6718,  96.6718,  96.6718],\n",
      "        [117.6558, 117.6558, 117.6558, 117.6558, 117.6558],\n",
      "        [114.4764, 114.4764, 114.4764, 114.4764, 114.4764],\n",
      "        [124.6505, 124.6505, 124.6505, 124.6505, 124.6505],\n",
      "        [ 90.3130,  90.3130,  90.3130,  90.3130,  90.3130]]) cost: 3930.417480\n",
      "Epoch    9/20 hypothesis: tensor([[103.2379, 103.2379, 103.2379, 103.2379, 103.2379],\n",
      "        [125.6471, 125.6471, 125.6471, 125.6471, 125.6471],\n",
      "        [122.2518, 122.2518, 122.2518, 122.2518, 122.2518],\n",
      "        [133.1169, 133.1169, 133.1169, 133.1169, 133.1169],\n",
      "        [ 96.4472,  96.4472,  96.4472,  96.4472,  96.4472]]) cost: 3052.943115\n",
      "Epoch   10/20 hypothesis: tensor([[109.0247, 109.0247, 109.0247, 109.0247, 109.0247],\n",
      "        [132.6901, 132.6901, 132.6901, 132.6901, 132.6901],\n",
      "        [129.1044, 129.1044, 129.1044, 129.1044, 129.1044],\n",
      "        [140.5786, 140.5786, 140.5786, 140.5786, 140.5786],\n",
      "        [101.8534, 101.8534, 101.8534, 101.8534, 101.8534]]) cost: 2371.368164\n",
      "Epoch   11/20 hypothesis: tensor([[114.1249, 114.1249, 114.1249, 114.1249, 114.1249],\n",
      "        [138.8973, 138.8973, 138.8973, 138.8973, 138.8973],\n",
      "        [135.1439, 135.1439, 135.1439, 135.1439, 135.1439],\n",
      "        [147.1548, 147.1548, 147.1548, 147.1548, 147.1548],\n",
      "        [106.6181, 106.6181, 106.6181, 106.6181, 106.6181]]) cost: 1841.955811\n",
      "Epoch   12/20 hypothesis: tensor([[118.6199, 118.6199, 118.6199, 118.6199, 118.6199],\n",
      "        [144.3680, 144.3680, 144.3680, 144.3680, 144.3680],\n",
      "        [140.4667, 140.4667, 140.4667, 140.4667, 140.4667],\n",
      "        [152.9507, 152.9507, 152.9507, 152.9507, 152.9507],\n",
      "        [110.8174, 110.8174, 110.8174, 110.8174, 110.8174]]) cost: 1430.735107\n",
      "Epoch   13/20 hypothesis: tensor([[122.5814, 122.5814, 122.5814, 122.5814, 122.5814],\n",
      "        [149.1894, 149.1894, 149.1894, 149.1894, 149.1894],\n",
      "        [145.1579, 145.1579, 145.1579, 145.1579, 145.1579],\n",
      "        [158.0587, 158.0587, 158.0587, 158.0587, 158.0587],\n",
      "        [114.5184, 114.5184, 114.5184, 114.5184, 114.5184]]) cost: 1111.320923\n",
      "Epoch   14/20 hypothesis: tensor([[126.0728, 126.0728, 126.0728, 126.0728, 126.0728],\n",
      "        [153.4387, 153.4387, 153.4387, 153.4387, 153.4387],\n",
      "        [149.2924, 149.2924, 149.2924, 149.2924, 149.2924],\n",
      "        [162.5607, 162.5607, 162.5607, 162.5607, 162.5607],\n",
      "        [117.7801, 117.7801, 117.7801, 117.7801, 117.7801]]) cost: 863.216187\n",
      "Epoch   15/20 hypothesis: tensor([[129.1499, 129.1499, 129.1499, 129.1499, 129.1499],\n",
      "        [157.1838, 157.1838, 157.1838, 157.1838, 157.1838],\n",
      "        [152.9362, 152.9362, 152.9362, 152.9362, 152.9362],\n",
      "        [166.5284, 166.5284, 166.5284, 166.5284, 166.5284],\n",
      "        [120.6549, 120.6549, 120.6549, 120.6549, 120.6549]]) cost: 670.501709\n",
      "Epoch   16/20 hypothesis: tensor([[131.8619, 131.8619, 131.8619, 131.8619, 131.8619],\n",
      "        [160.4844, 160.4844, 160.4844, 160.4844, 160.4844],\n",
      "        [156.1477, 156.1477, 156.1477, 156.1477, 156.1477],\n",
      "        [170.0252, 170.0252, 170.0252, 170.0252, 170.0252],\n",
      "        [123.1884, 123.1884, 123.1884, 123.1884, 123.1884]]) cost: 520.811035\n",
      "Epoch   17/20 hypothesis: tensor([[134.2521, 134.2521, 134.2521, 134.2521, 134.2521],\n",
      "        [163.3934, 163.3934, 163.3934, 163.3934, 163.3934],\n",
      "        [158.9780, 158.9780, 158.9780, 158.9780, 158.9780],\n",
      "        [173.1071, 173.1071, 173.1071, 173.1071, 173.1071],\n",
      "        [125.4214, 125.4214, 125.4214, 125.4214, 125.4214]]) cost: 404.538910\n",
      "Epoch   18/20 hypothesis: tensor([[136.3586, 136.3586, 136.3586, 136.3586, 136.3586],\n",
      "        [165.9571, 165.9571, 165.9571, 165.9571, 165.9571],\n",
      "        [161.4725, 161.4725, 161.4725, 161.4725, 161.4725],\n",
      "        [175.8233, 175.8233, 175.8233, 175.8233, 175.8233],\n",
      "        [127.3893, 127.3893, 127.3893, 127.3893, 127.3893]]) cost: 314.224579\n",
      "Epoch   19/20 hypothesis: tensor([[138.2151, 138.2151, 138.2151, 138.2151, 138.2151],\n",
      "        [168.2167, 168.2167, 168.2167, 168.2167, 168.2167],\n",
      "        [163.6710, 163.6710, 163.6710, 163.6710, 163.6710],\n",
      "        [178.2172, 178.2172, 178.2172, 178.2172, 178.2172],\n",
      "        [129.1237, 129.1237, 129.1237, 129.1237, 129.1237]]) cost: 244.073318\n",
      "Epoch   20/20 hypothesis: tensor([[139.8514, 139.8514, 139.8514, 139.8514, 139.8514],\n",
      "        [170.2081, 170.2081, 170.2081, 170.2081, 170.2081],\n",
      "        [165.6086, 165.6086, 165.6086, 165.6086, 165.6086],\n",
      "        [180.3270, 180.3270, 180.3270, 180.3270, 180.3270],\n",
      "        [130.6524, 130.6524, 130.6524, 130.6524, 130.6524]]) cost: 189.583282\n"
     ]
    }
   ],
   "source": [
    "x_train = torch.FloatTensor([[73, 80, 75],\n",
    "                             [93, 88, 93],\n",
    "                             [89, 91, 90],\n",
    "                             [96, 98, 100],\n",
    "                             [73, 66, 70]])\n",
    "\n",
    "x_train = torch.FloatTensor([[152], [185], [180], [196], [142]])\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "w = torch.zeros((1, 5), requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "\n",
    "optimizer = optim.SGD([w, b], lr=1e-5)\n",
    "\n",
    "nb_epochs = 20\n",
    "for epoch in range(nb_epochs+1):\n",
    "    hypothesis = x_train.matmul(w) + b\n",
    "    cost = torch.mean((hypothesis - y_train) ** 2)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(\"Epoch {:4d}/{} hypothesis: {} cost: {:.6f}\".format(\n",
    "        epoch, nb_epochs, hypothesis.squeeze().detach(), cost.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "model = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "import torch.nn.functional as F\n",
    "cost = F.mse_loss(prediction, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[0.5153]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.4414], requires_grad=True)]\n",
      "Epoch    0/2000 cost: 13.103541\n",
      "Epoch  100/2000 cost: 0.002791\n",
      "Epoch  200/2000 cost: 0.001724\n",
      "Epoch  300/2000 cost: 0.001066\n",
      "Epoch  400/2000 cost: 0.000658\n",
      "Epoch  500/2000 cost: 0.000407\n",
      "Epoch  600/2000 cost: 0.000251\n",
      "Epoch  700/2000 cost: 0.000155\n",
      "Epoch  800/2000 cost: 0.000096\n",
      "Epoch  900/2000 cost: 0.000059\n",
      "Epoch 1000/2000 cost: 0.000037\n",
      "Epoch 1100/2000 cost: 0.000023\n",
      "Epoch 1200/2000 cost: 0.000014\n",
      "Epoch 1300/2000 cost: 0.000009\n",
      "Epoch 1400/2000 cost: 0.000005\n",
      "Epoch 1500/2000 cost: 0.000003\n",
      "Epoch 1600/2000 cost: 0.000002\n",
      "Epoch 1700/2000 cost: 0.000001\n",
      "Epoch 1800/2000 cost: 0.000001\n",
      "Epoch 1900/2000 cost: 0.000000\n",
      "Epoch 2000/2000 cost: 0.000000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "x_train = torch.FloatTensor([[1], [2], [3]])\n",
    "y_train = torch.FloatTensor([[2], [4], [6]])\n",
    "\n",
    "model = nn.Linear(1, 1)\n",
    "print(list(model.parameters()))\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "nb_epochs = 2000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    prediction = model(x_train)\n",
    "    cost = F.mse_loss(prediction, y_train)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(\"Epoch {:4d}/{} cost: {:.6f}\".format(epoch, nb_epochs, cost.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[1.9994]], requires_grad=True), Parameter containing:\n",
      "tensor([0.0014], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "print(list(model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[ 0.2975, -0.2548, -0.1119]], requires_grad=True), Parameter containing:\n",
      "tensor([0.2710], requires_grad=True)]\n",
      "Epoch    0/2000 cost: 31667.597656\n",
      "Epoch  100/2000 cost: 0.225993\n",
      "Epoch  200/2000 cost: 0.223911\n",
      "Epoch  300/2000 cost: 0.221941\n",
      "Epoch  400/2000 cost: 0.220059\n",
      "Epoch  500/2000 cost: 0.218271\n",
      "Epoch  600/2000 cost: 0.216575\n",
      "Epoch  700/2000 cost: 0.214950\n",
      "Epoch  800/2000 cost: 0.213413\n",
      "Epoch  900/2000 cost: 0.211952\n",
      "Epoch 1000/2000 cost: 0.210560\n",
      "Epoch 1100/2000 cost: 0.209232\n",
      "Epoch 1200/2000 cost: 0.207967\n",
      "Epoch 1300/2000 cost: 0.206761\n",
      "Epoch 1400/2000 cost: 0.205619\n",
      "Epoch 1500/2000 cost: 0.204522\n",
      "Epoch 1600/2000 cost: 0.203484\n",
      "Epoch 1700/2000 cost: 0.202485\n",
      "Epoch 1800/2000 cost: 0.201542\n",
      "Epoch 1900/2000 cost: 0.200635\n",
      "Epoch 2000/2000 cost: 0.199769\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "x_train = torch.FloatTensor([[73, 80, 75],\n",
    "                             [93, 88, 93],\n",
    "                             [89, 91, 90],\n",
    "                             [96, 98, 100],\n",
    "                             [73, 66, 70]])\n",
    "\n",
    "y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])\n",
    "\n",
    "model = nn.Linear(3, 1)\n",
    "\n",
    "print(list(model.parameters()))\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-5)\n",
    "\n",
    "nb_epochs = 2000\n",
    "for epoch in range(nb_epochs +1):\n",
    "    prediction = model(x_train)\n",
    "\n",
    "    cost = F.mse_loss(prediction, y_train)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    cost.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 100 == 0 :\n",
    "        print(\"Epoch {:4d}/{} cost: {:.6f}\".format(epoch, nb_epochs, cost.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단순 선형회귀 모델\n",
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(1, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.Linear(x)\n",
    "\n",
    "model = LinearRegressionModel()\n",
    "\n",
    "# 다중 선형회귀 모델\n",
    "class MultivariatedLinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(3, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "model = MultivariatedLinearRegressionModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/5 Batch 1/3 cost: 9264.955078\n",
      "Epoch    0/5 Batch 2/3 cost: 1297.079834\n",
      "Epoch    0/5 Batch 3/3 cost: 892.334229\n",
      "Epoch    1/5 Batch 1/3 cost: 211.432770\n",
      "Epoch    1/5 Batch 2/3 cost: 50.638744\n",
      "Epoch    1/5 Batch 3/3 cost: 19.237814\n",
      "Epoch    2/5 Batch 1/3 cost: 9.107946\n",
      "Epoch    2/5 Batch 2/3 cost: 0.700511\n",
      "Epoch    2/5 Batch 3/3 cost: 0.570977\n",
      "Epoch    3/5 Batch 1/3 cost: 1.513565\n",
      "Epoch    3/5 Batch 2/3 cost: 0.128820\n",
      "Epoch    3/5 Batch 3/3 cost: 0.013758\n",
      "Epoch    4/5 Batch 1/3 cost: 0.035176\n",
      "Epoch    4/5 Batch 2/3 cost: 0.109691\n",
      "Epoch    4/5 Batch 3/3 cost: 2.018996\n",
      "Epoch    5/5 Batch 1/3 cost: 0.640522\n",
      "Epoch    5/5 Batch 2/3 cost: 0.505552\n",
      "Epoch    5/5 Batch 3/3 cost: 0.005554\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "x_train = torch.FloatTensor([[73, 80, 75],\n",
    "                             [93, 88, 93],\n",
    "                             [89, 91, 90],\n",
    "                             [96, 98, 100],\n",
    "                             [73, 66, 70]])\n",
    "\n",
    "y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])\n",
    "\n",
    "dataset = TensorDataset(x_train, y_train)\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "model = nn.Linear(3, 1)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-5)\n",
    "\n",
    "nb_epochs = 5\n",
    "\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    for batch_idx, samples in enumerate(dataloader):\n",
    "        x_train, y_train = samples\n",
    "\n",
    "        prediction = model(x_train)\n",
    "        cost = F.mse_loss(prediction, y_train)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "        print(\"Epoch {:4d}/{} Batch {}/{} cost: {:.6f}\".format(\n",
    "            epoch, nb_epochs, batch_idx+1, len(dataloader), cost.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (1672926917.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [109]\u001b[0;36m\u001b[0m\n\u001b[0;31m    def __len__(self):\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self):\n",
    "        # 데이터 전처리\n",
    "    def __len__(self):\n",
    "        # 데이터셋의 길이, 즉 샘플의 총 수를 적는 부분\n",
    "    def __getitem__(self, idx):\n",
    "        # 데이터셋에서 특정 1개의 샘플을 가져오는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/5 Batch 1/3 Cost: 27377.451172\n",
      "Epoch    0/5 Batch 2/3 Cost: 14383.236328\n",
      "Epoch    0/5 Batch 3/3 Cost: 1639.672852\n",
      "Epoch    1/5 Batch 1/3 Cost: 897.473389\n",
      "Epoch    1/5 Batch 2/3 Cost: 563.536865\n",
      "Epoch    1/5 Batch 3/3 Cost: 100.977417\n",
      "Epoch    2/5 Batch 1/3 Cost: 8.344945\n",
      "Epoch    2/5 Batch 2/3 Cost: 27.861610\n",
      "Epoch    2/5 Batch 3/3 Cost: 23.496368\n",
      "Epoch    3/5 Batch 1/3 Cost: 8.535181\n",
      "Epoch    3/5 Batch 2/3 Cost: 3.684837\n",
      "Epoch    3/5 Batch 3/3 Cost: 5.614924\n",
      "Epoch    4/5 Batch 1/3 Cost: 0.758023\n",
      "Epoch    4/5 Batch 2/3 Cost: 12.610233\n",
      "Epoch    4/5 Batch 3/3 Cost: 13.243709\n",
      "Epoch    5/5 Batch 1/3 Cost: 6.475133\n",
      "Epoch    5/5 Batch 2/3 Cost: 8.670555\n",
      "Epoch    5/5 Batch 3/3 Cost: 3.423539\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.x_data = [[73, 80, 75],\n",
    "                       [93, 88, 93],\n",
    "                       [89, 91, 90],\n",
    "                       [96, 98, 100],\n",
    "                       [73, 66, 70]]\n",
    "\n",
    "        self.y_data = [[152], [185], [180], [196], [142]]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.FloatTensor(self.x_data[idx])\n",
    "        y = torch.FloatTensor(self.y_data[idx])\n",
    "        return x, y\n",
    "\n",
    "dataset = CustomDataset()\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "model = torch.nn.Linear(3, 1)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-5)\n",
    "\n",
    "nb_epochs = 5\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    for batch_idx, samples in enumerate(dataloader):\n",
    "        x_train, y_train = samples\n",
    "        prediction = model(x_train)\n",
    "        cost = F.mse_loss(prediction, y_train)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "        print(\"Epoch {:4d}/{} Batch {}/{} Cost: {:.6f}\".format(epoch, nb_epochs, batch_idx+1, len(dataloader), cost.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 cost: 0.693147\n",
      "Epoch  100/1000 cost: 0.134722\n",
      "Epoch  200/1000 cost: 0.080643\n",
      "Epoch  300/1000 cost: 0.057900\n",
      "Epoch  400/1000 cost: 0.045300\n",
      "Epoch  500/1000 cost: 0.037261\n",
      "Epoch  600/1000 cost: 0.031673\n",
      "Epoch  700/1000 cost: 0.027556\n",
      "Epoch  800/1000 cost: 0.024394\n",
      "Epoch  900/1000 cost: 0.021888\n",
      "Epoch 1000/1000 cost: 0.019852\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "x_data = [[1, 2], [2, 3], [3, 1], [4, 3], [5, 3], [6, 2]]\n",
    "y_data = [[0], [0], [0], [1], [1], [1]]\n",
    "\n",
    "x_train = torch.FloatTensor(x_data)\n",
    "y_train = torch.FloatTensor(y_data)\n",
    "\n",
    "w = torch.zeros((2, 1), requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "\n",
    "optimizer = optim.SGD([w, b], lr=1)\n",
    "\n",
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs+1):\n",
    "    hypothesis = torch.sigmoid(x_train.matmul(w) + b)\n",
    "    cost = -(y_train*torch.log(hypothesis) + (1-y_train)*torch.log(1-hypothesis)).mean()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(\"Epoch {:4d}/{} cost: {:.6f}\".format(epoch, nb_epochs, cost.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4020],\n",
      "        [0.4147],\n",
      "        [0.6556],\n",
      "        [0.5948],\n",
      "        [0.6788],\n",
      "        [0.8061]], grad_fn=<SigmoidBackward0>)\n",
      "Epoch    0/100 Cost: 0.539713 Accuracy 83.33%\n",
      "Epoch   10/100 Cost: 0.614853 Accuracy 66.67%\n",
      "Epoch   20/100 Cost: 0.441875 Accuracy 66.67%\n",
      "Epoch   30/100 Cost: 0.373145 Accuracy 83.33%\n",
      "Epoch   40/100 Cost: 0.316358 Accuracy 83.33%\n",
      "Epoch   50/100 Cost: 0.266094 Accuracy 83.33%\n",
      "Epoch   60/100 Cost: 0.220498 Accuracy 100.00%\n",
      "Epoch   70/100 Cost: 0.182095 Accuracy 100.00%\n",
      "Epoch   80/100 Cost: 0.157299 Accuracy 100.00%\n",
      "Epoch   90/100 Cost: 0.144091 Accuracy 100.00%\n",
      "Epoch  100/100 Cost: 0.134272 Accuracy 100.00%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "x_data = [[1, 2], [2, 3], [3, 1], [4, 3], [5, 3], [6, 2]]\n",
    "y_data = [[0], [0], [0], [1], [1], [1]]\n",
    "x_train = torch.FloatTensor(x_data)\n",
    "y_train = torch.FloatTensor(y_data)\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(2, 1),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "\n",
    "print(model(x_train))\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=1)\n",
    "\n",
    "nb_epochs = 100\n",
    "for epoch in range(nb_epochs+1):\n",
    "    hypothesis = model(x_train)\n",
    "    cost = F.binary_cross_entropy(hypothesis, y_train)\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        prediction = hypothesis >= torch.FloatTensor([0.5])\n",
    "        correct_prediction = prediction.float() == y_train\n",
    "        accuracy = correct_prediction.sum().item() / len(correct_prediction)\n",
    "        print(\"Epoch {:4d}/{} Cost: {:.6f} Accuracy {:2.2f}%\".format(\n",
    "            epoch, nb_epochs, cost.item(), accuracy*100\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 3])\n",
      "Epoch    0/10000 cost: 1.098612\n",
      "Epoch 1000/10000 cost: 0.851076\n",
      "Epoch 2000/10000 cost: 0.785051\n",
      "Epoch 3000/10000 cost: 0.744691\n",
      "Epoch 4000/10000 cost: 0.714731\n",
      "Epoch 5000/10000 cost: 0.690764\n",
      "Epoch 6000/10000 cost: 0.670850\n",
      "Epoch 7000/10000 cost: 0.653893\n",
      "Epoch 8000/10000 cost: 0.639185\n",
      "Epoch 9000/10000 cost: 0.626237\n",
      "Epoch 10000/10000 cost: 0.614693\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)\n",
    "x_train = [[1, 2, 1, 1],\n",
    "           [2, 1, 3, 2],\n",
    "           [3, 1, 3, 4],\n",
    "           [4, 1, 5, 5],\n",
    "           [1, 7, 5, 5],\n",
    "           [1, 2, 5, 6],\n",
    "           [1, 6, 6, 6],\n",
    "           [1, 7, 7, 7]]\n",
    "y_train = [2, 2, 2, 1, 1, 1, 0, 0]\n",
    "\n",
    "x_train = torch.FloatTensor(x_train)\n",
    "y_train = torch.LongTensor(y_train)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "y_one_hot = torch.zeros(8, 3)\n",
    "y_one_hot.scatter_(1, y_train.unsqueeze(1), 1)\n",
    "print(y_one_hot.shape)\n",
    "\n",
    "w = torch.zeros((4, 3), requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "\n",
    "optimizer = optim.SGD([w, b], lr=0.001)\n",
    "\n",
    "nb_epochs = 10000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    z = x_train.matmul(w) + b\n",
    "    cost = F.cross_entropy(z, y_train)\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    if epoch % 1000 == 0:\n",
    "        print(\"Epoch {:4d}/{} cost: {:.6f}\".format(epoch, nb_epochs, cost.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.7273974418640137\n",
      "1000 0.6931471824645996\n",
      "2000 0.6931471824645996\n",
      "3000 0.6931471824645996\n",
      "4000 0.6931471824645996\n",
      "5000 0.6931471824645996\n",
      "6000 0.6931471824645996\n",
      "7000 0.6931471824645996\n",
      "8000 0.6931471824645996\n",
      "9000 0.6931471824645996\n",
      "10000 0.6931471824645996\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.manual_seed(777)\n",
    "if device == \"cuda\":\n",
    "    torch.cuda.manual_seed_all(777)\n",
    "\n",
    "x = torch.FloatTensor([[0,0], [0,1], [1,0], [1,1]]).to(device)\n",
    "y = torch.FloatTensor([[0], [1], [1], [0]]).to(device)\n",
    "\n",
    "linear = nn.Linear(2, 1, bias=True)\n",
    "sigmoid = nn.Sigmoid()\n",
    "model =  nn.Sequential(linear, sigmoid).to(device)\n",
    "\n",
    "criterion = torch.nn.BCELoss().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1)\n",
    "\n",
    "for step in range(10001):\n",
    "    optimizer.zero_grad()\n",
    "    hypothesis = model(x)\n",
    "    cost = criterion(hypothesis, y)\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    if step % 1000 == 0:\n",
    "        print(step, cost.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.6948983669281006\n",
      "1000 0.6931380033493042\n",
      "2000 0.6931172609329224\n",
      "3000 0.6930763721466064\n",
      "4000 0.6929103136062622\n",
      "5000 0.6820817589759827\n",
      "6000 0.0013030603295192122\n",
      "7000 0.00048370816512033343\n",
      "8000 0.0002889616880565882\n",
      "9000 0.00020376498287077993\n",
      "10000 0.00015648972475901246\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "torch.manual_seed(777)\n",
    "if device == \"cuda\":\n",
    "    torch.cuda.manual_seed_all(777)\n",
    "\n",
    "x = torch.FloatTensor([[0,0], [0,1], [1,0], [1,1]]).to(device)\n",
    "y = torch.FloatTensor([[0], [1], [1], [0]]).to(device)\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(2, 10, bias=True),\n",
    "    nn.Sigmoid(),\n",
    "    nn.Linear(10, 10, bias=True),\n",
    "    nn.Sigmoid(),\n",
    "    nn.Linear(10, 10, bias=True),\n",
    "    nn.Sigmoid(),\n",
    "    nn.Linear(10, 1, bias=True),\n",
    "    nn.Sigmoid()\n",
    "    ).to(device)\n",
    "\n",
    "criterion = torch.nn.BCELoss().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1)\n",
    "\n",
    "for epoch in range(10001):\n",
    "    optimizer.zero_grad()\n",
    "    hypothesis = model(x)\n",
    "\n",
    "    cost = criterion(hypothesis, y)\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 1000 == 0:\n",
    "        print(epoch, cost.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "다음 기기로 학습합니다 :  cuda\n",
      "Epoch:  0001 cost =  0.535150588\n",
      "Epoch:  0002 cost =  0.359577715\n",
      "Epoch:  0003 cost =  0.331264287\n",
      "Epoch:  0004 cost =  0.316404670\n",
      "Epoch:  0005 cost =  0.307107002\n",
      "Epoch:  0006 cost =  0.300456554\n",
      "Epoch:  0007 cost =  0.294933408\n",
      "Epoch:  0008 cost =  0.290956199\n",
      "Epoch:  0009 cost =  0.287074089\n",
      "Epoch:  0010 cost =  0.284515619\n",
      "Epoch:  0011 cost =  0.281914055\n",
      "Epoch:  0012 cost =  0.279526860\n",
      "Epoch:  0013 cost =  0.277636588\n",
      "Epoch:  0014 cost =  0.275874794\n",
      "Epoch:  0015 cost =  0.274422765\n",
      "Epoch:  0016 cost =  0.272883624\n",
      "Epoch:  0017 cost =  0.271629572\n",
      "Epoch:  0018 cost =  0.270609796\n",
      "Epoch:  0019 cost =  0.269295007\n",
      "Epoch:  0020 cost =  0.268277347\n",
      "Learning finished\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n",
    "print(\"다음 기기로 학습합니다 : \", device)\n",
    "\n",
    "random.seed(777)\n",
    "torch.manual_seed(777)\n",
    "if device == \"cuda\":\n",
    "    torch.cuda.manual_seed_all(777)\n",
    "\n",
    "mnist_train = dsets.MNIST(root = \"\",\n",
    "                          train = True,\n",
    "                          transform=transforms.ToTensor(),\n",
    "                          download=True)\n",
    "mnist_test  = dsets.MNIST(root=\"\",\n",
    "                          train = False,\n",
    "                          transform=transforms.ToTensor(),\n",
    "                          download=True)\n",
    "\n",
    "data_loader = DataLoader(dataset=mnist_train,\n",
    "                         batch_size=100,\n",
    "                         shuffle=True,\n",
    "                         drop_last=True)\n",
    "\n",
    "linear = nn.Linear(784, 10, bias=True).to(device)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.SGD(linear.parameters(), lr=0.1)\n",
    "\n",
    "training_epochs = 20\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = len(data_loader)\n",
    "\n",
    "    for x, y in data_loader:\n",
    "        x = x.view(-1, 28*28).to(device)\n",
    "        y = y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        hypothesis = linear(x)\n",
    "        cost = criterion(hypothesis, y)\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        avg_cost += cost / total_batch\n",
    "\n",
    "    print(\"Epoch: \", \"%04d\" % (epoch + 1), \"cost = \", \"{:.9f}\".format(avg_cost))\n",
    "\n",
    "print(\"Learning finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "다음 기기로 학습합니다 :  cuda\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [64, 32, 3, 3], expected input[100, 1, 28, 28] to have 32 channels, but got 1 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/sydney/Desktop/2022_SELF_STUDY/KETI/JetsonNano_CNN/torch_basic.ipynb Cell 49'\u001b[0m in \u001b[0;36m<cell line: 78>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sydney/Desktop/2022_SELF_STUDY/KETI/JetsonNano_CNN/torch_basic.ipynb#ch0000048?line=85'>86</a>\u001b[0m y \u001b[39m=\u001b[39m y\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sydney/Desktop/2022_SELF_STUDY/KETI/JetsonNano_CNN/torch_basic.ipynb#ch0000048?line=87'>88</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/sydney/Desktop/2022_SELF_STUDY/KETI/JetsonNano_CNN/torch_basic.ipynb#ch0000048?line=88'>89</a>\u001b[0m hypothesis \u001b[39m=\u001b[39m model(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sydney/Desktop/2022_SELF_STUDY/KETI/JetsonNano_CNN/torch_basic.ipynb#ch0000048?line=89'>90</a>\u001b[0m cost \u001b[39m=\u001b[39m criterion(hypothesis, y)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sydney/Desktop/2022_SELF_STUDY/KETI/JetsonNano_CNN/torch_basic.ipynb#ch0000048?line=90'>91</a>\u001b[0m acc \u001b[39m=\u001b[39m fn_acc(hypothesis, y)\n",
      "File \u001b[0;32m~/Desktop/2022_SELF_STUDY/딥러닝파이토치교과서/DeepLearningPytorch_venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/home/sydney/Desktop/2022_SELF_STUDY/KETI/JetsonNano_CNN/torch_basic.ipynb Cell 49'\u001b[0m in \u001b[0;36mCNN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sydney/Desktop/2022_SELF_STUDY/KETI/JetsonNano_CNN/torch_basic.ipynb#ch0000048?line=54'>55</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sydney/Desktop/2022_SELF_STUDY/KETI/JetsonNano_CNN/torch_basic.ipynb#ch0000048?line=55'>56</a>\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer1(x)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/sydney/Desktop/2022_SELF_STUDY/KETI/JetsonNano_CNN/torch_basic.ipynb#ch0000048?line=56'>57</a>\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayer2(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sydney/Desktop/2022_SELF_STUDY/KETI/JetsonNano_CNN/torch_basic.ipynb#ch0000048?line=57'>58</a>\u001b[0m     out \u001b[39m=\u001b[39m out\u001b[39m.\u001b[39mview(out\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m), \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sydney/Desktop/2022_SELF_STUDY/KETI/JetsonNano_CNN/torch_basic.ipynb#ch0000048?line=59'>60</a>\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc(out)\n",
      "File \u001b[0;32m~/Desktop/2022_SELF_STUDY/딥러닝파이토치교과서/DeepLearningPytorch_venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Desktop/2022_SELF_STUDY/딥러닝파이토치교과서/DeepLearningPytorch_venv/lib/python3.8/site-packages/torch/nn/modules/container.py:141\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    140\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 141\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    142\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/Desktop/2022_SELF_STUDY/딥러닝파이토치교과서/DeepLearningPytorch_venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Desktop/2022_SELF_STUDY/딥러닝파이토치교과서/DeepLearningPytorch_venv/lib/python3.8/site-packages/torch/nn/modules/conv.py:447\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 447\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/Desktop/2022_SELF_STUDY/딥러닝파이토치교과서/DeepLearningPytorch_venv/lib/python3.8/site-packages/torch/nn/modules/conv.py:443\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    440\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    441\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    442\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 443\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    444\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [64, 32, 3, 3], expected input[100, 1, 28, 28] to have 32 channels, but got 1 channels instead"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n",
    "print(\"다음 기기로 학습합니다 : \", device)\n",
    "\n",
    "random.seed(777)\n",
    "torch.manual_seed(777)\n",
    "if device == \"cuda\":\n",
    "    torch.cuda.manual_seed_all(777)\n",
    "\n",
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "\n",
    "mnist_train = dsets.MNIST(root = \"\",\n",
    "                          train = True,\n",
    "                          transform=transforms.ToTensor(),\n",
    "                          download=True)\n",
    "mnist_test  = dsets.MNIST(root=\"\",\n",
    "                          train = False,\n",
    "                          transform=transforms.ToTensor(),\n",
    "                          download=True)\n",
    "\n",
    "data_loader = DataLoader(dataset=mnist_train,\n",
    "                         batch_size=100,\n",
    "                         shuffle=True,\n",
    "                         drop_last=True)\n",
    "\n",
    "class CNN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        self.layer1 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "\n",
    "        self.layer2 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "\n",
    "        self.fc = torch.nn.Linear(7*7*64, 10, bias=True)\n",
    "\n",
    "        torch.nn.init.xavier_uniform_(self.fc.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(x)\n",
    "        out = out.view(out.size(0), -1)\n",
    "\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "model = CNN().to(device)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "total_batch = len(data_loader)\n",
    "\n",
    "num_data = len(data_loader.dataset)\n",
    "num_batch = np.ceil(num_data / batch_size)\n",
    "\n",
    "fn_acc = lambda pred, label:((pred.max(dim=1)[1]==label).type(torch.float)).mean()\n",
    "\n",
    "start_epoch = 0\n",
    "for epoch in range(start_epoch+1, training_epochs+1):\n",
    "    avg_cost = 0\n",
    "    total_batch = len(data_loader)\n",
    "    loss_arr = []\n",
    "    acc_arr = []\n",
    "\n",
    "    for batch, (x, y) in enumerate (data_loader, 1):\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        hypothesis = model(x)\n",
    "        cost = criterion(hypothesis, y)\n",
    "        acc = fn_acc(hypothesis, y)\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_arr += [cost.item()]\n",
    "        acc_arr += [acc.item()]\n",
    "        avg_cost += cost / total_batch\n",
    "        if batch % 100 == 0:\n",
    "            print(\"Train: EPOCH %04d/%04d | BATCH %04d/%04d | LOSS: %.4f | ACC %.4f\" \n",
    "            %(epoch, training_epochs, batch, num_batch, np.mean(loss_arr), np.mean(acc_arr)))\n",
    "print(\"[Epoch: {:>4} cost: {:>.9}\".format(epoch+1, avg_cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def save(ckpt_dir, optim, epoch):\n",
    "    if not os.path.exists(ckpt_dir):\n",
    "        os.mkdirs(ckpt_dir)\n",
    "        torch.save({\"net\":net.state_dict(),\n",
    "                    \"optim\": optim.state_dict()},\n",
    "                    \"./%s/model_epoch%.dpth\" %(ckpt_dir, epoch))\n",
    "\n",
    "def load(ckpt_dir, net, optim):\n",
    "    ckpt_list =  os.listdir(ckpt_dir)\n",
    "    ckpt_list.sort()\n",
    "\n",
    "    dict_model = torch.load()\n",
    "\n",
    "    net.load_state_dict(dict_model[\"net\"])\n",
    "    optim.load_state_dic(dict_model[\"optim\"])\n",
    "    \n",
    "    return net, optim"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('DeepLearningPytorch_venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "be2f8b69f8ed956b49f7f6168c2e75f7de12de890ba504612cd4d4039b5ebf3e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
